{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116bf675",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import sys\n",
    "import yaml\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "\n",
    "\n",
    "#get custom functions to use\n",
    "sys.path.append('../')\n",
    "from myutils import get_video_info, capture_frames, train_valid_test_split\n",
    "from myutils import track_droplet\n",
    "from plot_utils import add_speed, plot_speed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c56040",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Create Particle Detection Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16465f5",
   "metadata": {},
   "source": [
    "- Some global variables we will keep using in this tutorial are below. We will demonstrate everything on the walking droplet experiment with 3 walkers. You should be able to train your own model by following the same steps for your own experiment\n",
    "\n",
    "\n",
    "- **To create your own model using this tutorial, make sure to create your own folder instead of working on \"tutorial_data\" folder. Otherwise, your results will overwrite on the actual repository data**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a29220",
   "metadata": {},
   "outputs": [],
   "source": [
    "#three-droplet experiment video\n",
    "video_path = \"../dataset/videos/three_droplet.mp4\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb75c244",
   "metadata": {},
   "source": [
    "Assuming you have the experiment video, first step is to save some sample frames from the video source. It is always a good idea to capture only the relevant components of the experiment such as experiment corral and walkers etc. Thus you may want to crop your video before creating training data. You can use [Avidemux](https://avidemux.sourceforge.net/download.html0) which is an amazing online tool. First call **\"get_video_info\"** helper function to get an idea about the video source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b31dec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_video_info(video_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8bfa3b",
   "metadata": {},
   "source": [
    "Based on the info above, decide how many frames to be captured. For example, if we aim for 180 frames in total, we need to capture one frame in every (total_frames/180)/frame_rate = 1.7 second. **\"capture_frames\"** function does this job and captures one frame in every *save_interval* second and save to *image_dir*. Make sure you are happy with the captures images in *image_dir*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3db964",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save dir for captures frames\n",
    "image_dir = \"raw_images/\"\n",
    "capture_frames(video_path=video_path,image_dir = image_dir,save_interval=1.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0c26b0",
   "metadata": {},
   "source": [
    "We will now create a traning/validation/testing dataset from these frames. The most common ratio is 70/20/10. **train_valid_test_split** helper function does this job. This function creates a folder structure for train/valid/test data as \n",
    "\n",
    "                    dest_dir/datasets/train/images and dest_dir/datasets/train/labels\n",
    "                    dest_dir/datasets/valid/images and dest_dir/datasets/valid/labels \n",
    "                    dest_dir/datasets/test/images and dest_dir/datasets/test/labels \n",
    "\n",
    "At this point, *labels* folders are empty. The same function also creates  *sample.yaml and dummy_test.yaml* files inside * dest_dir/datasets*\n",
    "that are necessary for traninng and testing our YOLO models. Make sure these files are all in place. \"datasets\" folder is there due to an ongoing bug in YOLOv8, not a big deal. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c76b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_valid_test_split(image_dir=image_dir,dest_dir='./',\n",
    "                       train_ratio=0.7, valid_ratio=0.2, test_ratio=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a57518",
   "metadata": {},
   "source": [
    "Now, we are ready to annotate the all the images to create training, validation and testing data we will use for training our YOLO model. To do so we will be using free online annotation tool **LabelImg**. It is super easy to use, here is a quick tutorial [here](https://www.youtube.com/watch?v=VsZvT69Ssbs). Make sure to switch YOLO format at the beginning. Annotate each images in train,valid and test images in *root_dir/X/images* folders and save them to respective *root_dir/X/labels* folder. Simply use \"droplet\" as a default label name in the app.\n",
    "\n",
    "\n",
    "Just run the following cell to access its user interface. In the app, zoom-in to droplets to create high quality bounding boxes. We have already done this before. Inspect the folders inside each directory. Notice that each text file in *\"labels\"* folder has exactly the same name with its corresponding image file in *\"images\"* folder.\n",
    "\n",
    "\n",
    "If you have multiple experiments to carry out, check out Sec-6 in this notebook to get an idea about approximately how many training images you should use. You can save up quite a bit annotation time with that approach. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c247b82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i labelImg/labelImg.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d3b78d",
   "metadata": {},
   "source": [
    "## Traning YOLO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b178970b-7873-4119-af3e-dcf975974aa5",
   "metadata": {},
   "source": [
    "- If we are done with annotation, we are ready to train and test our YOLOv8 model. We already annotated \"three droplet\" experiment, we will use the very same files here. If you are here just to see how this notebook works, copy train-valid-test folders within *datasets/three_droplet* folder in the main page and paste them into \"tutorial_data/datasets\". \n",
    "\n",
    "- The following option for model training is self-explanatory. We will save all the YOLO tranining results into *\"project/name\"* folder. In the same folder, you will find tons of useful information. The ones we will definetely use is /weights/best.pt' which is the best model of our tranining. We will load and use the best model in the rest of the notebook. \n",
    "\n",
    "\n",
    "- You can experiment with different pretrained models; yolov8n, yolov8s, yolov8m, yolov8l, yolov8x(increasing in size )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482551e5-8a1f-4487-9044-4bccfeb489ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#main yaml directory\n",
    "data = 'datasets/sample.yaml'\n",
    "\n",
    "#save all yolo results here\n",
    "yolo_results = 'yolo_results'\n",
    "\n",
    "#save tranining results to yolo_results/experiment_name\n",
    "experiment_name = \"sample_project\"\n",
    "\n",
    "#overwrite the traning results for different trials\n",
    "exist_ok = True\n",
    "\n",
    "#number of epochs\n",
    "epochs = 1\n",
    "\n",
    "#reproducibility\n",
    "seed = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99b8d79-8938-447f-98b8-fcef2da49a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load and train the model\n",
    "model = YOLO('yolov8n.yaml')\n",
    "model = YOLO('yolov8n.pt') \n",
    "\n",
    "model.train(data=data, epochs=epochs,project=yolo_results,name=experiment_name,exist_ok=exist_ok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b210b7bc-2043-42a7-a370-9d47bac12d53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b11d936-93a7-4108-bd13-88eaa5062729",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test the model on testing images\n",
    "data_test = 'datasets/dummy_test.yaml'\n",
    "model.val(data = data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0deb4c48",
   "metadata": {},
   "source": [
    "- **If *mAP50* value on testing data is way below 0.90, that means the model did not learn enough thus will likely to fail in real-time tracker. Based on our experience, first start by increasing the the number of epochs to a high number say 400. If the behaviour is the same, increasing the number of training images may help. Add 20-30 images/labels to the training data. You can also increase the batch_size**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd88247-6f13-462f-aba2-27e03abffad4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9fb2b7fe",
   "metadata": {},
   "source": [
    "## Droplet/Intruder Tracking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88927283",
   "metadata": {},
   "source": [
    "- Once the model is trained, you will find the best model at  f\"{project}/{name}/weights/best.pt\" as noted above. To visualize and save the tracking results, simply modify the following cell. **track_droplet** displays the tracking in real-time and saves the trajectories to *save_dir/save_name.csv* which we will discuss in a bit. Ignore if you get \"QObject::moveToThread\" error. The same function also saves the tracking video in the same directory.\n",
    "\n",
    "\n",
    "- Make sure to properly enter all the arguments. If you spot any false positives, try increasing the threshold slightly. You can always interrupt the simulation by pressing \"q\" on your keyboard. All information is saved in real-time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0743bb-29b5-4c95-af82-7ac09ed88646",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_path = f\"{yolo_results}/{experiment_name}/weights/best.pt\"\n",
    "model = YOLO(model_path) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d8e302",
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of particles in the experiment\n",
    "num_particle = 3\n",
    "\n",
    "#experiment video to be tracked, defined at the top\n",
    "video_path = video_path\n",
    "\n",
    "#accept detections only above this confidance\n",
    "conf_thresold = 0.45\n",
    "\n",
    "#save tracks and tracking video here\n",
    "save_dir = 'tracking_results'\n",
    "\n",
    "#name your experiment\n",
    "save_name = experiment_name\n",
    "\n",
    "#show trajectory or just bounding box with IDs\n",
    "show_trace = True\n",
    "\n",
    "# #run the tracker\n",
    "track_droplet(model=model, num_particle=num_particle, video_path=video_path, conf_thresold=conf_thresold,\n",
    "                                        save_dir=save_dir, save_name=save_name, show_trace=show_trace)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43cbb37-4b93-4ca9-a553-ea9e070eebdd",
   "metadata": {},
   "source": [
    "                    experiment: sample_project detection_rate: 8909/9033 = 98.627%\n",
    "                    trajectories saved to tracking_results/sample_project.csv\n",
    "\n",
    "                    (8909, 9033, 185.93071365356445)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e942b7d7-4eb6-4ebf-8ff5-3d79b2af5894",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b43368c4",
   "metadata": {},
   "source": [
    "## Inspecting Results and Some Post-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081ef590",
   "metadata": {},
   "source": [
    "- Let's inspect the results regarding our original experiment.We will start by loading the dataframe we saved. You can analyze this data in a way you wish. \n",
    "\n",
    "\n",
    "- **frame_id, time, x, y, c** columns refers to the frame number, time stamp(sec), x-position,y-position of each individual droplet/intruder tracked. detected=1 means we detected precisely 3 particles(which is different in other experiments) in that frame with confidance score 0.45 we set above. This ensures we dont get false positives. detected=0 rows has only frame_id and time properties. It can be useful for diagnosis purposes. \n",
    "\n",
    "\n",
    "- For example, using the plot function below, we can overlay the position and the flow of the object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2380bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(f\"{save_dir}/{save_name}.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a98a39-f7f0-4150-b025-cea84b73fb11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8edd017-7658-4219-b70b-cbf20bf5f8f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db0f0af-f164-4588-ad9b-5e4a0b0c1469",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_with_speed = add_speed(data=data,num_particle=3)\n",
    "df_with_speed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131a5f62-a0b8-4ead-a912-c53d352d6708",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e365dc3b-e0a7-49f9-8c90-fc842cb5df3c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#square: inital point; circle:terminal point, you can enter num_particle=3 to see all speed maps\n",
    "plot_speed(data=data,num_particle=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac21d456-4395-4dc2-8e50-74233a607c98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd1a490-03b1-4413-8f4a-f9943abe250f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063dccd6-0f76-45fe-ba5b-02b36de5a975",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1aee67ff",
   "metadata": {},
   "source": [
    "## Use Different Trackers with YOLOv8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa04207",
   "metadata": {},
   "source": [
    "- Any tracking-by-detection method can operate on YOLOv8 detection. [yolov8_tracker](https://github.com/mikel-brostrom/yolov8_tracking) by mikel-brostrom is an amazing tool to directly obtain the tracks from SOTA trackers on top of YOLOv8. As fas as I can see, 'strongsort', 'deepocsort', 'ocsort', 'bytetrack' and 'botsort' are supported there. Following is a simple implemantation. You can experiment with different tracker. \n",
    "\n",
    "\n",
    "- As we discussed in our paper, **these models suffer from multiple ID switches in all multiple droplet experiments. Thus, their results cannot be used by any means for multiple walking droplet experiments**. However, they can be useful for single particle tracking or some other experiments. It is not a good practice to run the command line arguments from Jupyter but this is just a demo. \n",
    "\n",
    "- Restart the notebook if you encounter any error in the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80fef8d-c00c-43b1-8957-32622e574625",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# List of tracking methods ['strongsort', 'deepocsort', 'ocsort', 'bytetrack', 'botsort']\n",
    "tracking_method = 'strongsort'\n",
    "\n",
    "# Root directory for all tutorial data\n",
    "track_dir = 'sota_tracker'\n",
    "\n",
    "# Accept detections above this\n",
    "conf_thresold = 0.45\n",
    "\n",
    "#experiment name\n",
    "exp_name = 'strongsort_track'\n",
    "\n",
    "video_path = video_path\n",
    "\n",
    "os.system(f\"python ../yolov8_tracking/track.py --yolo-weights {model_path} --tracking-method {tracking_method}\\\n",
    "          --source {video_path} --conf-thres {conf_thresold} \\\n",
    "          --project {track_dir} --name {exp_name} \\\n",
    "          --show-vid --save-txt --save-vid \\\n",
    "          \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21593d13-07c7-4d5d-a1b8-5053100edf66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e7c4aa-29a9-480b-ae2e-5b30e19ce40a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7877d1f4",
   "metadata": {},
   "source": [
    "                                               THANK YOU FOR CHECKING OUT!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": false,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "275px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "toc-autonumbering": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
