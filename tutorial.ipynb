{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91bfc95d",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Preparing-Dataset\" data-toc-modified-id=\"Preparing-Dataset-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Preparing Dataset</a></span></li><li><span><a href=\"#Data-Annotation\" data-toc-modified-id=\"Data-Annotation-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Data Annotation</a></span></li><li><span><a href=\"#Traning-YOLO-for-Droplet/Intruder-Detection\" data-toc-modified-id=\"Traning-YOLO-for-Droplet/Intruder-Detection-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Traning YOLO for Droplet/Intruder Detection</a></span></li><li><span><a href=\"#Droplet/Intruder-Tracking\" data-toc-modified-id=\"Droplet/Intruder-Tracking-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Droplet/Intruder Tracking</a></span></li><li><span><a href=\"#Inspecting-Results-and-Some-Post-Processing\" data-toc-modified-id=\"Inspecting-Results-and-Some-Post-Processing-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Inspecting Results and Some Post-Processing</a></span></li><li><span><a href=\"#How-Many-Training-Images-Do-I-Need?\" data-toc-modified-id=\"How-Many-Training-Images-Do-I-Need?-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>How Many Training Images Do I Need?</a></span></li><li><span><a href=\"#Tracking-with-StrongSORT\" data-toc-modified-id=\"Tracking-with-StrongSORT-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Tracking with StrongSORT</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116bf675",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from myutils import GetVideoInfo\n",
    "from myutils import CaptureFrames\n",
    "from myutils import TrackDroplet\n",
    "from myutils import TrainValidTestSplit\n",
    "from myutils import OptimumTrainImages\n",
    "from myutils import TrackMultipleExperiments\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c56040",
   "metadata": {},
   "source": [
    "## Preparing Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16465f5",
   "metadata": {},
   "source": [
    "- Some global variables we will keep using in this tutorial are below. We will demonstrate everything on the walking droplet experiment with 3 walkers. You should be able to train your own model by following the same steps for your own experiment\n",
    "\n",
    "\n",
    "- **To create your own model using this tutorial, make sure to create your own folder instead of working on \"tutorial_data\" folder. Otherwise, your results will overwrite on the actual repository data**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a29220",
   "metadata": {},
   "outputs": [],
   "source": [
    "#project to be conducted\n",
    "project_name = \"three_droplet\"\n",
    "\n",
    "#root_dir for all tutorial data\n",
    "project_dir = \"tutorial_data\"\n",
    "\n",
    "#images to be used for YOLO model\n",
    "raw_image_dir = \"tutorial_data/raw_images/\"\n",
    "\n",
    "#image/label pairs for train/valid/test\n",
    "data_dir = \"tutorial_data/annotations/\"\n",
    "\n",
    "#all experiment videos are here\n",
    "video_root_dir = \"datasets/videos/\"\n",
    "\n",
    "#three-droplet experiment video\n",
    "video_path = \"datasets/videos/three_droplet.mp4\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb75c244",
   "metadata": {},
   "source": [
    "- Assuming you have the experiment video, first step is to save some sample frames from the video source. It is always a good idea to capture only the relevant components of the experiment such as experiment corral and walkers etc. Thus you may want to crop your video before creating training data. You can use [Avidemux](https://avidemux.sourceforge.net/download.html0) which is an amazing online tool. \n",
    "\n",
    "\n",
    "- First call **\"GetVideoInfo\"** helper function to get an idea about the video source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b31dec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = \"datasets/videos/three_droplet.mp4\"\n",
    "GetVideoInfo(video_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8bfa3b",
   "metadata": {},
   "source": [
    "- Based on the info above, decide how many frames to be captured. For example, we aim for 180 frames in total, we need to capture one frame in every (total_frames/180)/frame_rate = 1.7 second. \n",
    "\n",
    "\n",
    "- **\"CaptureFrames\"** function captures one frame in every *save_interval* second and save to *image_dir*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3db964",
   "metadata": {},
   "outputs": [],
   "source": [
    "CaptureFrames(video_path=video_path,image_dir = raw_image_dir,save_interval=1.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0c26b0",
   "metadata": {},
   "source": [
    "- We will now create a traning/validation/testing dataset from these frame. The most common ratio is 70/20/10. \n",
    "\n",
    "\n",
    "- **TrainValidTestSplit** helper function does this job. This function first creates a folder structure for train/valid/test data as \n",
    "\n",
    "        root_dir/X/images and root_dir/X/labels. \n",
    "\n",
    "- We then get images from *image_dir* and save to these folders accordingly based on the provided ratios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c76b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainValidTestSplit(image_dir=raw_image_dir, \n",
    "                    root_dir=data_dir,train_ratio=0.7, valid_ratio=0.2, test_ratio=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4fbf30",
   "metadata": {},
   "source": [
    "## Data Annotation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a57518",
   "metadata": {},
   "source": [
    "- Now, we are ready to annotate the all the images to create training, validation and testing data we will use for training our YOLO model. To do so we will be using free online annotation tool **LabelImg**. \n",
    "\n",
    "\n",
    "- It is super easy to use, here is a quick tutorial [here](https://www.youtube.com/watch?v=VsZvT69Ssbs). Make sure to switch YOLO format at the beginning. Annotate each images in train,valid and test images in *root_dir/X/images* folders and save them to respective *root_dir/X/labels* folder. You can simply use \"droplet\" as a default label name in the app.\n",
    "\n",
    "\n",
    "- Just run the following cell to access its user interface. In the app, zoom-in to droplets to create high quality bounding boxes. We have already done this before. Inspect the folders inside each directory. Notice that each text file in *\"labels\"* folder has exactly the same name with its corresponding image file in *\"images\"* folder.\n",
    "\n",
    "\n",
    "- If you have multiple experiments to carry out, check out Sec-6 in this notebook to get an idea about approximately how many training images you should use. You can save up quite a bit annotation time with that approach "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c247b82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i labelImg/labelImg.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728b6709",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "29d3b78d",
   "metadata": {},
   "source": [
    "## Traning YOLO for Droplet/Intruder Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0c0d5a",
   "metadata": {},
   "source": [
    "- Now we are ready for model traning. First of all, find the *\"tutorial_data.yml\"* file in *\"yolov5/custom_data\"* folder and change the path variable to the directory including our train/valid/test folders. Without properly setting up this file, YOLO cannot access our data.\n",
    "\n",
    "\n",
    "- Once we are done, run the following cell to train your model. It would be a better practice to run it from terminal but it does the job anyway.\n",
    "\n",
    "\n",
    "- Briefly, it will train \"yolov5s\" architecture for 100 epochs using Adam optimizer on our data and save the results in *\"project_dir/project_name\"* folder. \n",
    "\n",
    "\n",
    "- By inspecting *mAP50* values, we can easily see the model is learning very quickly as values near 1 points to perfect performance. \n",
    "\n",
    "\n",
    "- **The best model will be located at *\"project_dir/project_name/weights/best.pt\"* . Training results can be found at *\"project_dir/project_name/results.csv\"***. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a1778b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = \"yolov5/custom_data/tutorial_data.yml\"\n",
    "epoch = 150\n",
    "project_dir = project_dir\n",
    "optimizer = \"Adam\"\n",
    "batch_size = 32 #default YOLO\n",
    "os.system(f\"python  yolov5/train.py --data {data} --weights yolov5/yolov5s.pt \\\n",
    "          --epoch {epoch} --optimizer {optimizer} --batch-size {batch_size} \\\n",
    "          --project {project_dir} --name {project_name} \\\n",
    "          --cache --exist-ok --seed 0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0deb4c48",
   "metadata": {},
   "source": [
    "- Now let's test the best model on our test data. \"mAP50\" value looks pretty good. it indicates that we will most likely obtain very high detection rate when we process the actual experiment video. This comes in the next section. \n",
    "\n",
    "- **If *mAP50* value on testing data is way below 0.90, that means the model is did not learn enough thus fails to generalize to unseen data. Based on our experience, first start by increasing the the number of epochs to a high number say 400. If the behaviour is the same, increasing the number of training images may help. Add 20-30 images/labels to the training data. You can also increase the batch_size**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1686d6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = f\"{project_dir}/{project_name}/weights/best.pt\"\n",
    "os.system(f\"python yolov5/val.py --data {data}  --weights {model_path} --task test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb2b7fe",
   "metadata": {},
   "source": [
    "## Droplet/Intruder Tracking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88927283",
   "metadata": {},
   "source": [
    "- Once the model is trained, you will find the best model at *\"tutorial_data/myproject/weights/best.pt\"* as noted above.\n",
    "\n",
    "\n",
    "- To visualize and save the tracking results, simply modify the following cell. **TrackDroplet** returns number of detected frames, total frames and a dataframe including positions, velocity, confidence scores for each individual droplet/intruder in the video source. This dataframe is saved to *\"save_dir\"* as a *\"save_name.csv\"* in **real time**. Ignore if you get \"QObject::moveToThread\" error. It also saves the tracking video in the same directory.\n",
    "\n",
    "\n",
    "- Make sure to properly enter \"number of objects(droplets/intruders) and video_path, model_path etc. If you spot any false positives, try increasing the threshold slightly. You can always interrupt the simulation by pressing \"q\" on your keyboard. All information is saved in real-time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d8e302",
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of droplet(s)/intruder(s)\n",
    "nd = 3\n",
    "\n",
    "#best YOLO Pytorch model path\n",
    "model_path = f\"{project_dir}/{project_name}/weights/best.pt\"\n",
    "model = torch.hub.load('ultralytics/yolov5', 'custom', path=model_path)\n",
    "\n",
    "#accept if only all detections are above this thresold\n",
    "conf_thresold = 0.45\n",
    "\n",
    "\n",
    "#False: show only bounding box, True: show trajectory\n",
    "show_trace = True\n",
    "\n",
    "detected,total_frame,df = TrackDroplet(model= model,conf_thresold=conf_thresold,nd=nd,\n",
    "                                    video_path=video_path, save_dir=project_dir,\n",
    "                                    save_name=project_name, show_trace=show_trace)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9aa189",
   "metadata": {},
   "source": [
    "- If you have a model trained for multiple experiments, you can also those experiments all at once using **\"TrackMultipleExperiments\"** functions. As outlined in the paper **\"best_droplet.pt\"** is the model we trained for all droplet experiments. The code below shows how to use it to track multiple experiments. At the end, we save frame detection rates for each experiment to save_dir/name.csv. You can always interrupt the simulation by pressing \"q\" on your keyboard.\n",
    "\n",
    "\n",
    "\n",
    "- Make sure the keys in exp_dict is exactly the same with the video names in *video_root_dir* and provide the number of droplet/intruder associated with the corresponding video source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc5d23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#experiment names and object number(s)\n",
    "exp_dict = {'control':1, 'lights_off':1}\n",
    "\n",
    "#root directory having all the videos\n",
    "video_root_dir = video_root_dir\n",
    "\n",
    "#best YOLO Pytorch model path\n",
    "model = torch.hub.load('ultralytics/yolov5', 'custom', path=\"best_droplet.pt\")\n",
    "\n",
    "\n",
    "#detect above this value\n",
    "conf_thresold = 0.45\n",
    "\n",
    "#save dir\n",
    "save_dir = project_dir\n",
    "\n",
    "\n",
    "#save name for dataframe\n",
    "name = \"frame_detection_rates\"\n",
    "\n",
    "#True shows the trajectory\n",
    "show_trace = False\n",
    "\n",
    "#track exps in exp_dict\n",
    "TrackMultipleExperiments(exp_dict=exp_dict,video_root_dir=video_root_dir,model=model,\n",
    "                         conf_thresold=conf_thresold,\n",
    "                         save_dir=save_dir, name=name, show_trace=False\n",
    "                         )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43368c4",
   "metadata": {},
   "source": [
    "## Inspecting Results and Some Post-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081ef590",
   "metadata": {},
   "source": [
    "- Let's inspect the results regarding our original experiment.We will start by loading the dataframe we saved. Or you can directly use it as it is return by  **TrackDroplet** function. This was the major goal of this tutorial. You can analyze this data in a way you wish. \n",
    "\n",
    "\n",
    "- **frame_id, time, x, y, c, dx,dy speed** columns refers to the frame number, time stamp(sec), x-position,y-position,x-velocity,y-velocity and speed of each individual droplet/intruder tracked. \n",
    "\n",
    "\n",
    "\n",
    "- For example, using the plot function below, we can overlay the position and the flow of the object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2380bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(f\"{project_dir}/{project_name}.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6350d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetDynamics(df,nd):    \n",
    "    xlist = [f\"x{i}\" for i in range(1,nd+1)]\n",
    "    dxlist = [f\"dx{i}\" for i in range(1,nd+1)]\n",
    "\n",
    "    ylist = [f\"y{i}\" for i in range(1,nd+1)]\n",
    "    dylist = [f\"dy{i}\" for i in range(1,nd+1)]\n",
    "\n",
    "    slist = [f\"speed{i}\" for i in range(1,nd+1)]\n",
    "\n",
    "    #get stuff column-wise\n",
    "    t = df[\"time\"].to_numpy()\n",
    "    xc = df[xlist].to_numpy()\n",
    "    dx = df[dxlist].to_numpy()\n",
    "\n",
    "    yc = df[ylist].to_numpy()\n",
    "    dy = df[dylist].to_numpy()\n",
    "\n",
    "    speed = df[slist].to_numpy()\n",
    "    \n",
    "    return t, xc, yc, dx, dy, speed\n",
    "\n",
    "def PlotFlow(nd,df,sample_interval=5):\n",
    "    _, xc, yc, _, _, _ = GetDynamics(df,nd)\n",
    "    fig,ax = plt.subplots(3,1,figsize=(8,18),sharex=True)\n",
    "    for i in range(nd):\n",
    "        X = xc[:,i]\n",
    "        Y = yc[:,i]\n",
    "        x = X[0:-1:sample_interval]\n",
    "        y = Y[0:-1:sample_interval]\n",
    "        dx = np.diff(x)\n",
    "        dy = np.diff(y)\n",
    "        dx1 = np.append(dx, 0)\n",
    "        dy1 = -np.append(dy, 0)\n",
    "        ax[i].plot(X, Y, linestyle='-', color='tomato')\n",
    "        ax[i].quiver(x, y, dx1, dy1, color='blue', units='width')\n",
    "        ax[i].plot(x[0], y[0], 'ks', label='initial point', markersize=15)\n",
    "        ax[i].plot(x[-1], y[-1], 'ko', label='terminal point', markersize=15)\n",
    "        ax[i].invert_yaxis()\n",
    "        ax[i].legend(loc='upper left',labelspacing = 1.5)\n",
    "    plt.show()\n",
    "    \n",
    "#restart and run again if you dont see images. YOLO has a tiny conflict with  plt.show()\n",
    "%matplotlib inline\n",
    "PlotFlow(nd=3,df=df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f0c18b",
   "metadata": {},
   "source": [
    "## How Many Training Images Do I Need?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf973a9",
   "metadata": {},
   "source": [
    "- We cannot give a definitive answer this question but we can make a quick experiment. \n",
    "\n",
    "\n",
    "- We have a folder \"data_dir = tutorial_data/annotations\", including all of our training data. We are uncertain as to whether the amount of training images is adequate to create an effective model, or if it is excessive. The later item is important if we are to repeat similar experiments multiple times. So the idea is the following;\n",
    "\n",
    "\n",
    "- Train the model with increasing number of training images by keeping 70%/20% train/valid ratio but keep the same number of testing images. Then monitor mAP scores vs number of training images. Let's say we have 70/20/10 train/valid/test images in our original dataset. We can train the model with 20/6/10, 40/12/10, 60/18/10 partitions and check how mAPs are changing for each case.\n",
    "\n",
    "\n",
    "- We are looking for some sort of asymptotic behaviour of  mAP0.5. We can then pick the number of images slightly after asymptotic behaviour started. This should give us a good estimate on the optimal number of images for our dataset so that we do not need to spend huge amount of time to annotate lots of images.  \n",
    "\n",
    "\n",
    "- To  much talking, **OptimumTrainImages** should do the job. Pick *max_image_number* images as an upper bound. This should not exceed the number of training images you initially prepared. We will test each case between *start_image_num* and final_imag_num with *num_interval* intervals. Pick a reasonable *epoch* number for each cycle. Recall that this is not the actual training, we would like to get a quick estimate. Thus 50 should do the job.. Outcome is  a dataframe in the *\"project_dir\"* with columns *num_train_image,mAP@[0.5],mAP@[0.5..0.95]*. The function will give you a plot anyway but you can use that file as well. This simulation should take around 25 minutes.\n",
    "\n",
    "\n",
    "- The final plot indicates that we should get pretty much the same performance just by using slightly more than 60 images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68945323",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = data_dir\n",
    "project_dir = project_dir\n",
    "max_image_number = 120\n",
    "start_image_num = 5\n",
    "final_imag_num = max_image_number\n",
    "num_interval = 5\n",
    "epoch  = 50\n",
    "save_name = \"test_scores\"\n",
    "OptimumTrainImages(data_dir=data_dir,project_dir=project_dir,\n",
    "                   max_image_number=max_image_number,start_image_num=start_image_num,\n",
    "                   final_image_num=final_imag_num, \n",
    "                   num_interval=num_interval,epoch=epoch,save_name=save_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e572298c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(f\"{project_dir}/{save_name}.csv\")\n",
    "num_train = df['num_train_image']\n",
    "mAP_05 = df['mAP@0.5']\n",
    "mAP_0595 = df['mAP@0.5..0.95']\n",
    "fig,ax = plt.subplots(figsize=(8,6))\n",
    "ax.plot(num_train, mAP_05, label='mAP@0.5', linestyle='--',marker='o')\n",
    "ax.plot(num_train, mAP_0595, label='mAP@0.5:0.95',marker='o')\n",
    "ax.set_xlabel('#training images')\n",
    "ax.set_ylabel('score')\n",
    "ax.grid(True)\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aee67ff",
   "metadata": {},
   "source": [
    "## Tracking with StrongSORT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa04207",
   "metadata": {},
   "source": [
    "- We also discussed tracking with StrongSORT in our paper. Once the YOLO model is ready, we can simply run the following cell with the usual arguments. Notice that StrongSORT suffers from multiple ID switches. \n",
    "\n",
    "\n",
    "\n",
    "- This notebook is just a demonstration, we highly recommend running the following command from terminal using as it will be way remarkably faster.\n",
    "\n",
    "        python Yolov5_StrongSORT_OSNet/track_erdi.py --yolo-weights \"best_droplet.pt\" --source \"datasets/videos/three_droplet.mp4\" --conf-thres 0.45 --show-vid --config-strongsort \"Yolov5_StrongSORT_OSNet/strong_sort/configs/strong_sort.yaml\"\n",
    "\n",
    "\n",
    "        \n",
    "- Tracking video and tracks in MOT format will be saved to *project_dir/project_name* folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0c20a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = f\"{project_dir}/{project_name}/weights/best.pt\"\n",
    "video_dir = video_path\n",
    "conf_thresold = 0.45\n",
    "project_dir = project_dir\n",
    "sort_dir = \"SORT_tracks\"\n",
    "\n",
    "os.system(f\"python Yolov5_StrongSORT_OSNet/track.py --yolo-weights {model_path} \\\n",
    "      --source {video_dir} --conf-thres {conf_thresold} \\\n",
    "      --project {project_dir} --name {sort_dir} \\\n",
    "      --show-vid --save-txt --save-vid \\\n",
    "      --config-strongsort Yolov5_StrongSORT_OSNet/strong_sort/configs/strong_sort.yaml\\\n",
    "      \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7877d1f4",
   "metadata": {},
   "source": [
    "                                               THANK YOU FOR CHECKING OUT!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": false,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "275px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
